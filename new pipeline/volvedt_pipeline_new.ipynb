{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/eb/4a3642e971f404d69d4f6fa3885559d67562801b99d7592487f1ecc4e017/pip-20.3.3-py2.py3-none-any.whl (1.5MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5MB 7.3MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "\u001b[33m  WARNING: The scripts pip, pip3 and pip3.6 are installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed pip-20.3.3\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (1.4.1)\n",
      "Collecting matplotlib==3.2.2\n",
      "  Downloading matplotlib-3.2.2-cp36-cp36m-manylinux1_x86_64.whl (12.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.4 MB 6.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.2.2) (1.18.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.2.2) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.2.2) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.2.2) (2.4.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.2.2) (0.10.0)\n",
      "Collecting pandas==0.24.2\n",
      "  Downloading pandas-0.24.2-cp36-cp36m-manylinux1_x86_64.whl (10.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.1 MB 49.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas==0.24.2) (2019.3)\n",
      "Collecting scikit-learn==0.23.1\n",
      "  Downloading scikit_learn-0.23.1-cp36-cp36m-manylinux1_x86_64.whl (6.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.8 MB 38.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting statsmodels==0.12.0\n",
      "  Downloading statsmodels-0.12.0-cp36-cp36m-manylinux1_x86_64.whl (9.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.5 MB 41.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib==3.2.2) (1.13.0)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-1.0.0-py3-none-any.whl (302 kB)\n",
      "\u001b[K     |████████████████████████████████| 302 kB 54.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib==3.2.2) (44.0.0)\n",
      "Collecting patsy>=0.5\n",
      "  Downloading patsy-0.5.1-py2.py3-none-any.whl (231 kB)\n",
      "\u001b[K     |████████████████████████████████| 231 kB 57.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Collecting lasio\n",
      "  Downloading lasio-0.28-py3-none-any.whl (39 kB)\n",
      "Installing collected packages: threadpoolctl, patsy, pandas, joblib, statsmodels, scikit-learn, matplotlib, lasio\n",
      "\u001b[33m  WARNING: The scripts las2excel, las2excelbulk, lasio and lasversionconvert are installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed joblib-1.0.0 lasio-0.28 matplotlib-3.2.2 pandas-0.24.2 patsy-0.5.1 scikit-learn-0.23.1 statsmodels-0.12.0 threadpoolctl-2.1.0\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install --user --upgrade pip\n",
    "\n",
    "!pip3 install lasio pandas==0.24.2 matplotlib==3.2.2 scipy==1.4.1 statsmodels==0.12.0 scikit-learn==0.23.1 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Collecting kfp\n",
      "  Downloading kfp-1.2.0.tar.gz (165 kB)\n",
      "\u001b[K     |████████████████████████████████| 165 kB 8.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from kfp) (5.3)\n",
      "Requirement already satisfied: google-cloud-storage>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from kfp) (1.25.0)\n",
      "Requirement already satisfied: kubernetes<12.0.0,>=8.0.0 in /usr/local/lib/python3.6/dist-packages (from kfp) (10.0.1)\n",
      "Requirement already satisfied: google-auth>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from kfp) (1.10.0)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from kfp) (1.2.2)\n",
      "Requirement already satisfied: jsonschema>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from kfp) (3.2.0)\n",
      "Collecting docstring-parser>=0.7.3\n",
      "  Downloading docstring_parser-0.7.3.tar.gz (13 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (4.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (44.0.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (4.0.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (1.13.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (0.2.8)\n",
      "Requirement already satisfied: google-resumable-media<0.6dev,>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage>=1.13.0->kfp) (0.5.0)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage>=1.13.0->kfp) (1.3.0)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (1.16.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (2.22.0)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (3.11.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (1.51.0)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (2019.3)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kfp) (1.4.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kfp) (0.15.7)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kfp) (19.3.0)\n",
      "Collecting kfp-pipeline-spec<0.2.0,>=0.1.0\n",
      "  Downloading kfp_pipeline_spec-0.1.3.1-py3-none-any.whl (11 kB)\n",
      "Collecting kfp-server-api<2.0.0,>=1.1.2\n",
      "  Downloading kfp-server-api-1.2.0.tar.gz (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 3.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: urllib3>=1.15 in /usr/local/lib/python3.6/dist-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (1.25.7)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (2019.11.28)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (2.8.1)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.6/dist-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (0.57.0)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.6/dist-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.6.1->kfp) (0.4.8)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (2.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (3.0.4)\n",
      "Collecting requests_toolbelt>=0.8.0\n",
      "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 3.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting click\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "\u001b[K     |████████████████████████████████| 82 kB 1.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting Deprecated\n",
      "  Downloading Deprecated-1.2.10-py2.py3-none-any.whl (8.7 kB)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from Deprecated->kfp) (1.11.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->jsonschema>=3.0.1->kfp) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.6/dist-packages (from zipp>=0.5->importlib-metadata->jsonschema>=3.0.1->kfp) (8.0.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib->kubernetes<12.0.0,>=8.0.0->kfp) (3.1.0)\n",
      "Collecting strip-hints\n",
      "  Downloading strip-hints-0.1.9.tar.gz (30 kB)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from strip-hints->kfp) (0.30.0)\n",
      "Collecting tabulate\n",
      "  Downloading tabulate-0.8.7-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: kfp, docstring-parser, kfp-server-api, strip-hints\n",
      "  Building wheel for kfp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kfp: filename=kfp-1.2.0-py3-none-any.whl size=229698 sha256=402990d87c6e5d98cd18d1682871857cd9ae24b703a2e3ed56429a2583b69eb5\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/4f/45/50/6fb5f06208872028e99b604f97cbc51467797e9495bf49fef8\n",
      "  Building wheel for docstring-parser (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docstring-parser: filename=docstring_parser-0.7.3-py3-none-any.whl size=19230 sha256=176d641639349e89d93904b2d4192731785ff12411c3549d56baaa5f54618131\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/32/63/02/bb6eebc5261f10a6de3dcf26336a7b2b8b8dc8cacb6c00f75f\n",
      "  Building wheel for kfp-server-api (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kfp-server-api: filename=kfp_server_api-1.2.0-py3-none-any.whl size=108925 sha256=a33051d24e2b69a7616e8db916a10d12072711ff9c2044d1e7593af5bf669acb\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/8e/79/78/b1b20e26f9a6237db0fa0d41008e52f9b06d1d70b8464cddbc\n",
      "  Building wheel for strip-hints (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for strip-hints: filename=strip_hints-0.1.9-py2.py3-none-any.whl size=24671 sha256=76f8a10716594b462ba8cb2f86d69a708e78c524c452d8b4190b64365da42ac3\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/21/6d/fa/7ed7c0560e1ef39ebabd5cc0241e7fca711660bae1ad752e2b\n",
      "Successfully built kfp docstring-parser kfp-server-api strip-hints\n",
      "Installing collected packages: tabulate, strip-hints, requests-toolbelt, kfp-server-api, kfp-pipeline-spec, docstring-parser, Deprecated, click, kfp\n",
      "\u001b[33m  WARNING: The script tabulate is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script strip-hints is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The scripts dsl-compile, dsl-compile-v2 and kfp are installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed Deprecated-1.2.10 click-7.1.2 docstring-parser-0.7.3 kfp-1.2.0 kfp-pipeline-spec-0.1.3.1 kfp-server-api-1.2.0 requests-toolbelt-0.9.1 strip-hints-0.1.9 tabulate-0.8.7\n"
     ]
    }
   ],
   "source": [
    "!pip3 install kfp --upgrade --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if the install was successful\n",
    "!which dsl-compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Kubeflow SDK\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.components as comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where the outputs are stored\n",
    "out_dir = \"/home/jovyan/Volve_ML/data/\"\n",
    "holdout = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tranform(data_path):\n",
    "    \n",
    "    import pickle\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.24.2'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.23.1'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'ipython'])\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import PowerTransformer\n",
    "    \n",
    "    # Download the dataset and split into training and test data. \n",
    "    data = pd.read_csv(\"https://raw.githubusercontent.com/Josepholaidepetro/Volve_ML/main/data/train.csv\")\n",
    "    \n",
    "    # Preprocess\n",
    "    data.drop(['DEPTH', 'BS', 'RD', 'ROP', 'RM', 'DRHO'], axis=1, inplace=True)\n",
    "    \n",
    "    # If Nan, drop\n",
    "    data.dropna(inplace=True)\n",
    "    \n",
    "    # transform the RT to logarithmic\n",
    "    data['RT'] = np.log10(data['RT'])\n",
    "    \n",
    "    # perform a yeo-johnson transform of the train dataset\n",
    "    ptrain = PowerTransformer(method='yeo-johnson')\n",
    "    train_df_yj = ptrain.fit_transform(data.drop('DT', axis=1))\n",
    "    train_df_yj_norm = pd.DataFrame(train_df_yj, columns=data.columns.drop('DT'))\n",
    "    y_train = data['DT']\n",
    "    \n",
    "    #Save the train_data as a pickle file to be used by the predict component.\n",
    "    with open(f'{data_path}/train_data', 'wb') as f:\n",
    "        pickle.dump((train_df_yj_norm,  y_train), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tranform(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tranform(data_path):\n",
    "    \n",
    "    import pickle\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.24.2'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.23.1'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'ipython'])\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import PowerTransformer\n",
    "    \n",
    "    # Download the dataset and split into training and test data. \n",
    "    data = pd.read_csv(\"https://raw.githubusercontent.com/Josepholaidepetro/Volve_ML/main/data/test.csv\")\n",
    "    \n",
    "    # Preprocess\n",
    "    data.drop(['DEPTH', 'BS', 'ROP', 'DRHO'], axis=1, inplace=True)\n",
    "    \n",
    "    # If Nan, drop\n",
    "    data.dropna(inplace=True)\n",
    "    \n",
    "    # transform the RT to logarithmic\n",
    "    data['RT'] = np.log10(data['RT'])\n",
    "    \n",
    "    # perform a yeo-johnson transform of the train dataset\n",
    "    ptest = PowerTransformer(method='yeo-johnson')\n",
    "    test_df_yj = ptest.fit_transform(data.drop('DT', axis=1))\n",
    "    test_df_yj_norm = pd.DataFrame(test_df_yj, columns=data.columns.drop('DT'))\n",
    "    y_test = data['DT']\n",
    "    \n",
    "    #Save the test_data as a pickle file to be used by the predict component.\n",
    "    with open(f'{data_path}/test_data', 'wb') as f:\n",
    "        pickle.dump((test_df_yj_norm,  y_test), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.6/site-packages/sklearn/preprocessing/_data.py:2995: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
     ]
    }
   ],
   "source": [
    "test_tranform(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Outlier_removal(data_path):\n",
    "    \n",
    "    import pickle\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.24.2'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.23.1'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'ipython'])\n",
    "    \n",
    "    from sklearn.svm import OneClassSVM\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Load and unpack the train_data\n",
    "    with open(f'{data_path}/train_data','rb') as f:\n",
    "        train_data = pickle.load(f)\n",
    "    # Separate the train_df_yj_norm from y_train.\n",
    "    train_df_yj_norm,  y_train = train_data\n",
    "\n",
    "    # Method 4: One-class SVM\n",
    "    svm = OneClassSVM(nu=0.13)\n",
    "    yhat = svm.fit_predict(train_df_yj_norm)\n",
    "    mask = yhat != -1\n",
    "    train_df_svm = train_df_yj_norm[mask]\n",
    "    y_train_svm = y_train[mask]\n",
    "\n",
    "    # prepare train data for modelling\n",
    "    X_train = train_df_svm.copy().drop(['label'], axis=1)\n",
    "    y_train = y_train_svm.copy()\n",
    "    train_df_svm['DT'] = y_train\n",
    "    \n",
    "    #Save the train_data as a pickle file to be used by the predict component.\n",
    "    with open(f'{data_path}/train_data2', 'wb') as f:\n",
    "        pickle.dump((X_train,  y_train), f)\n",
    "        \n",
    "    #Save the train_data to be used for splitting as a pickle file to be used by the predict component.\n",
    "    with open(f'{data_path}/train_split', 'wb') as f:\n",
    "        pickle.dump((train_df_svm), f)\n",
    "    \n",
    "    print('Outlier removed successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier removed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "Outlier_removal(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_tranform(data_path):\n",
    "    \n",
    "    import pickle\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.24.2'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.23.1'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'ipython'])\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import PowerTransformer\n",
    "    \n",
    "    # Load and unpack the train_data\n",
    "    with open(f'{data_path}/train_split','rb') as f:\n",
    "        train_split = pickle.load(f)\n",
    "    X_train = train_split\n",
    "    \n",
    "    # Separate the data into X_train and y_train.\n",
    "    X_train2 = X_train[X_train['label'] < 0].drop(['label', 'DT'], axis=1)\n",
    "    y_train2 = X_train[X_train['label'] < 0]['DT']\n",
    "    \n",
    "    # Separate the data into X_valid and y_valid.\n",
    "    X_valid = X_train[X_train['label'] > 0].drop(['label', 'DT'], axis=1)\n",
    "    y_valid = X_train[X_train['label'] > 0]['DT']\n",
    "    \n",
    "    #Save the train_data and valid_data as a pickle file to be used by the predict component.\n",
    "    with open(f'{data_path}/train_data3', 'wb') as f:\n",
    "        pickle.dump((X_valid,  y_valid), f)\n",
    "        \n",
    "    #Save the train_data and valid_data as a pickle file to be used by the predict component.\n",
    "    with open(f'{data_path}/train_data4', 'wb') as f:\n",
    "        pickle.dump((X_train2,  y_train2), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_tranform(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_building(data_path, holdout):\n",
    "    \n",
    "    import pickle\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.24.2'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.23.1'])\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.ensemble import ExtraTreesRegressor\n",
    "    \n",
    "    # Load and unpack the full train_data2\n",
    "    with open(f'{data_path}/train_data2','rb') as f:\n",
    "        train_data2 = pickle.load(f)\n",
    "    # Separate the X_train from y_train.\n",
    "    X_train,  y_train = train_data2\n",
    "    \n",
    "    # Load and unpack the train_data4\n",
    "    with open(f'{data_path}/train_data4','rb') as f:\n",
    "        train_data4 = pickle.load(f)\n",
    "    # Separate the X_train from y_train.\n",
    "    X_train4,  y_train4 = train_data4\n",
    "    \n",
    "    # Load and unpack the valid_data\n",
    "    with open(f'{data_path}/train_data3','rb') as f:\n",
    "        train_data3 = pickle.load(f)\n",
    "    # Separate the train_df_yj_norm from y_train.\n",
    "    X_valid,  y_valid = train_data3\n",
    "    \n",
    "    # Load and unpack the test_data\n",
    "    with open(f'{data_path}/test_data','rb') as f:\n",
    "        test_data = pickle.load(f)\n",
    "    # Separate the train_df_yj_norm from y_train.\n",
    "    X_test, y_test = test_data\n",
    "    \n",
    "    # Define the model.\n",
    "    model = ExtraTreesRegressor(n_estimators=800, max_depth=6, random_state=21)\n",
    "    \n",
    "    if holdout is False:\n",
    "        # Run a training job\n",
    "        model.fit(X_train, y_train)\n",
    "    else:\n",
    "        # Run a training job\n",
    "        model.fit(X_train4, y_train4)\n",
    "\n",
    "    \n",
    "    #Save the model to the designated \n",
    "    filename = f'{data_path}/modelo.pkl'\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "    \n",
    "    #Save holdout\n",
    "    with open(f'{data_path}/holdout', 'wb') as f:\n",
    "        pickle.dump((holdout), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = model_building(out_dir, holdout=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluate(data_path):\n",
    "    \n",
    "    import pickle\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.24.2'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.23.1'])\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.ensemble import ExtraTreesRegressor\n",
    "    \n",
    "    file = f'{data_path}/holdout'\n",
    "    holdout = pickle.load(open(file, 'rb'))\n",
    "    \n",
    "    # Load the saved model\n",
    "    filename = f'{data_path}/modelo.pkl'\n",
    "    model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "    # Load and unpack the test_data\n",
    "    with open(f'{data_path}/test_data','rb') as f:\n",
    "        test_data = pickle.load(f)\n",
    "    # Separate the X_test from y_test.\n",
    "    X_test, y_test = test_data\n",
    "    \n",
    "    \n",
    "    # Load and unpack the valid_data\n",
    "    with open(f'{data_path}/train_data3','rb') as f:\n",
    "        train_data3 = pickle.load(f)\n",
    "    # Separate the train_df_yj_norm from y_train.\n",
    "    X_valid,  y_valid = train_data3\n",
    "    \n",
    "    if holdout is False:\n",
    "        #Evaluate the model and print the results\n",
    "        score = model.score(X_test, y_test)\n",
    "        print(\"R-squared of Well 3: {}\".format(score))\n",
    "    else:\n",
    "        score = model.score(X_valid,y_valid)\n",
    "        print(\"R-squared of Well 3: {}\".format(score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared of Well 3: 0.8683283360096595\n"
     ]
    }
   ],
   "source": [
    "model_evaluate(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data_path):\n",
    "    \n",
    "    import pickle\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.24.2'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.23.1'])\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.ensemble import ExtraTreesRegressor\n",
    "    \n",
    "    # Load the holdout variable\n",
    "    file = f'{data_path}/holdout'\n",
    "    holdout = pickle.load(open(file, 'rb'))\n",
    "    \n",
    "    # Load the saved model\n",
    "    filename = f'{data_path}/modelo.pkl'\n",
    "    regressor = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "    if holdout is False:\n",
    "        # Load and unpack the test_data\n",
    "        with open(f'{data_path}/test_data','rb') as f:\n",
    "            test_data = pickle.load(f)\n",
    "        # Separate the X_test from y_test.\n",
    "        test_df_yj_norm,  y_test = test_data\n",
    "\n",
    "        # make predictions.\n",
    "        y_pred = regressor.predict(test_df_yj_norm)\n",
    "    else:\n",
    "        # Load and unpack the test_data\n",
    "        with open(f'{data_path}/test_data','rb') as f:\n",
    "            test_data = pickle.load(f)\n",
    "        # Separate the X_test from y_test.\n",
    "        test_df_yj_norm,  y_test = test_data\n",
    "\n",
    "        # make predictions.\n",
    "        y_pred = regressor.predict(test_df_yj_norm)\n",
    "        \n",
    "    \n",
    "    with open(f'{data_path}/result1', 'wb') as f:\n",
    "        pickle.dump(y_pred, f)\n",
    "        \n",
    "    with open(f'{data_path}/result.txt', 'w') as result:\n",
    "        result.write(\" Prediction: {}, Actual: {} \".format(y_pred,y_test))\n",
    "    \n",
    "    print('Prediction has be saved successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and predict lightweight components.\n",
    "train_tranform_op = comp.func_to_container_op(train_tranform , base_image = \"tensorflow/tensorflow:latest-gpu-py3\")\n",
    "test_tranform_op = comp.func_to_container_op(test_tranform , base_image = \"tensorflow/tensorflow:latest-gpu-py3\")\n",
    "valid_tranform_op = comp.func_to_container_op(valid_tranform , base_image = \"tensorflow/tensorflow:latest-gpu-py3\")\n",
    "Outlier_removal_op = comp.func_to_container_op(Outlier_removal , base_image = \"tensorflow/tensorflow:latest-gpu-py3\")\n",
    "train_op = comp.func_to_container_op(model_building , base_image = \"tensorflow/tensorflow:latest-gpu-py3\")\n",
    "valid_op = comp.func_to_container_op(model_evaluate , base_image = \"tensorflow/tensorflow:latest-gpu-py3\")\n",
    "predict_op = comp.func_to_container_op(predict , base_image = \"tensorflow/tensorflow:latest-gpu-py3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Kubeflow Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a client to enable communication with the Pipelines API server.\n",
    "client = kfp.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "@dsl.pipeline(\n",
    "   name='Sonic Log Prediction Pipeline',\n",
    "   description='An ML pipeline that performs Sonic Log Prediction prediction.'\n",
    ")\n",
    "\n",
    "# Define parameters to be fed into pipeline\n",
    "def sonic_container_pipeline(\n",
    "    data_path: str,\n",
    "    holdout: bool\n",
    "    ):\n",
    "    \n",
    "    # Define volume to share data between components.\n",
    "    vop = dsl.VolumeOp(\n",
    "    name=\"create_volume\",\n",
    "    resource_name=\"data-volume\", \n",
    "    size=\"1Gi\", \n",
    "    modes=dsl.VOLUME_MODE_RWO) # read write one\n",
    "    \n",
    "    # Create deploy component.\n",
    "    train_tranform_container = train_tranform_op(data_path) \\\n",
    "                                    .add_pvolumes({data_path: vop.volume})\n",
    "\n",
    "    # Create data transformation component.\n",
    "    test_tranform_container = test_tranform_op(data_path) \\\n",
    "                                    .add_pvolumes({data_path: train_tranform_container.pvolume})\n",
    "    \n",
    "    # Create data transformation component.\n",
    "    valid_tranform_container = valid_tranform_op(data_path) \\\n",
    "                                    .add_pvolumes({data_path: test_tranform_container.pvolume})\n",
    "    \n",
    "    # Create data transformation component.\n",
    "    Outlier_removal_container = Outlier_removal_op(data_path) \\\n",
    "                                    .add_pvolumes({data_path: valid_tranform_container.pvolume})\n",
    "    \n",
    "    # Create model training component.\n",
    "    train_container = train_op(data_path, holdout) \\\n",
    "                                    .add_pvolumes({data_path: Outlier_removal_container.pvolume})\n",
    "    \n",
    "    # Create model validation component.\n",
    "    valid_container = valid_op(data_path) \\\n",
    "                                    .add_pvolumes({data_path: train_container.pvolume})\n",
    "    \n",
    "    # Create model training component.\n",
    "    predict_container = predict_op(data_path) \\\n",
    "                                    .add_pvolumes({data_path: valid_container.pvolume})\n",
    "    \n",
    "    \n",
    "    # Print the result of the prediction\n",
    "    churn_result_container = dsl.ContainerOp(\n",
    "        name=\"print_prediction\",\n",
    "        image='library/bash:4.4.23', # Image tag for the Docker container to be used.\n",
    "        pvolumes={data_path: predict_container.pvolume}, # the name displayed for the component execution during runtime.\n",
    "        arguments=['cat', f'{data_path}/result.txt']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_func = sonic_container_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = out_dir # mount your filesystems or directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.6/site-packages/kfp/dsl/_container_op.py:1039: FutureWarning: Please create reusable components instead of constructing ContainerOp instances directly. Reusable components are shareable, portable and have compatibility and support guarantees. Please see the documentation: https://www.kubeflow.org/docs/pipelines/sdk/component-development/#writing-your-component-definition-file The components can be created manually (or, in case of python, using kfp.components.create_component_from_func or func_to_container_op) and then loaded using kfp.components.load_component_from_file, load_component_from_uri or load_component_from_text: https://kubeflow-pipelines.readthedocs.io/en/stable/source/kfp.components.html#kfp.components.load_component_from_file\n",
      "  category=FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/a6a89285-9b6f-4bd7-a4c0-c17aaa42ce48\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/68f5b649-9053-4c8a-a79e-4ba18b46c477\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment_name = 'dt_prediction_kubeflow'\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "\n",
    "arguments = {\"data_path\":DATA_PATH}\n",
    "\n",
    "# Compile pipeline to generate compressed YAML definition of the pipeline.\n",
    "kfp.compiler.Compiler().compile(pipeline_func, '{}.zip'.format(experiment_name))\n",
    "\n",
    "# Submit pipeline directly from pipeline function\n",
    "run_result = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                                  experiment_name=experiment_name, \n",
    "                                                  run_name=run_name, \n",
    "                                                  arguments=arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://localhost:8888/pipeline#/experiments/details/644a33c5-3114-4b3c-b6de-3b0b6467eafa\n",
    "\n",
    "http://localhost:8888/pipeline#/runs/details/2aadb518-ead4-4035-90ea-011a44bfe16d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
